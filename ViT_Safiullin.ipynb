{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","celltoolbar":"Edit Metadata","colab":{"name":"ViT_Safiullin.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"ORlPPFxjJloq"},"source":["# Visual Transformer(part 1)\n","https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1\n","\n","Github links:\n","\n","* [google-research/vision_transformer](https://github.com/google-research/vision_transformer)\n","*  [lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch)\n","* [lukemelas/PyTorch-Pretrained-ViT](https://github.com/lukemelas/PyTorch-Pretrained-ViT)"]},{"cell_type":"markdown","metadata":{"id":"6bq3EvdxMnXR"},"source":["## Задание\n","1. Выбрать удобный для вас датасет, например, любое соревнование по CV. Пример приведен на датасете Dogs vs Cats Data\n","2. Взять любую предобученную модель VIT и затем сделать finetuning на вашем датасете\n","3. Сравнить по качеству с любой CNN моделью"]},{"cell_type":"markdown","metadata":{"id":"cUffLYHSU76m"},"source":[" * Histopathologic Cancer Detection - https://www.kaggle.com/c/histopathologic-cancer-detection  \n"," \n"," \n"," \"In this competition, you must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans. The data for this competition is a slightly modified version of the PatchCamelyon (PCam) benchmark dataset (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).\"\n"," \n"," \n"]},{"cell_type":"code","metadata":{"id":"s_z7TuN_RIOK","executionInfo":{"status":"ok","timestamp":1615242732871,"user_tz":-180,"elapsed":5157,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["!pip -q install vit_pytorch linformer"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IDT1Xwv9RZXb"},"source":["## Import Libraries"]},{"cell_type":"code","metadata":{"id":"qq-e52uORIVg","executionInfo":{"status":"ok","timestamp":1615242737053,"user_tz":-180,"elapsed":9335,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["from __future__ import print_function\n","\n","import glob\n","from itertools import chain\n","import os\n","import random\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import datasets, transforms\n","from tqdm.notebook import tqdm\n","import random\n","\n","\n","\n","from torchvision import datasets, transforms\n","import cv2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pV-E0IFHRITw","executionInfo":{"status":"ok","timestamp":1615242737054,"user_tz":-180,"elapsed":9193,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}},"outputId":"431d7d4f-753b-4d58-aabc-c0d435f74e20"},"source":["print(f\"Torch: {torch.__version__}\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Torch: 1.8.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mSO-6jwwlYzO","executionInfo":{"status":"ok","timestamp":1615242737054,"user_tz":-180,"elapsed":9192,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["seed = 42\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_everything(seed)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZS5Jp9SBRgQr","executionInfo":{"status":"ok","timestamp":1615242737054,"user_tz":-180,"elapsed":9190,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["device = 'cuda'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WK7eSEEmw-Bs"},"source":["## From kaggle to colab"]},{"cell_type":"code","metadata":{"id":"OJSJQJG7xDMI","executionInfo":{"status":"ok","timestamp":1615242737055,"user_tz":-180,"elapsed":9190,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# !pip install -q kaggle\n","# !pip install --upgrade --force-reinstall --no-deps kaggle\n","#  # https://www.kaggle.com/Robohant/account\n","#  !get kaggle.json - CREATE NEW API TOKEN(your profile)\n","#  from google.colab import files \n","#  files.upload()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jImfBHqkxFtq","executionInfo":{"status":"ok","timestamp":1615242737055,"user_tz":-180,"elapsed":9182,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}},"outputId":"59303c47-12f6-4be3-dd48-92bc6d965f4d"},"source":["!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pB80KwLSxTR1","executionInfo":{"status":"ok","timestamp":1615242737055,"user_tz":-180,"elapsed":9181,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# !rm -r ~/.kaggle\n","# ! mkdir ~/.kaggle\n","# ! cp kaggle.json ~/.kaggle/\n","# ! chmod 600 ~/.kaggle/kaggle.json\n","# #! kaggle datasets list"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"oquLn2KrxiqC","executionInfo":{"status":"ok","timestamp":1615242737056,"user_tz":-180,"elapsed":9180,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# !kaggle competitions download -c histopathologic-cancer-detection\n","# !unzip histopathologic-cancer-detection.zip  \n","# !unzip -q train.zip\n","# !unzip -q test.zip\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZEIyZaYXRkZF"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"lvw64WGURgxK","executionInfo":{"status":"error","timestamp":1615242738090,"user_tz":-180,"elapsed":10213,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}},"outputId":"4d52a37c-fd0b-4edf-9af3-c3d4d57ea6e3","colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["\n","train_path = 'train'\n","test_path = 'test'\n","labels = pd.read_csv('train_labels.csv')\n"],"execution_count":10,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d3fb8bbd4e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_labels.csv'"]}]},{"cell_type":"code","metadata":{"id":"IFVvqhfm96ly","executionInfo":{"status":"aborted","timestamp":1615242738065,"user_tz":-180,"elapsed":10186,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["\n","class MyDataset(Dataset):\n","    def __init__(self, df_data, data_dir = './', transform=None, pathe = None):\n","        super().__init__()\n","        self.df = df_data.values\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.pathe = os.path.join(self.data_dir,(self.df[0][0])+'.tif')\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        img_name,label = self.df[index]\n","        img_path = os.path.join(self.data_dir, img_name+'.tif')\n","        image = cv2.imread(img_path)\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZhYDJXk2SRDu"},"source":["## Image Augumentation"]},{"cell_type":"code","metadata":{"id":"O84JewnT96lz","executionInfo":{"status":"aborted","timestamp":1615242738065,"user_tz":-180,"elapsed":10185,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# Random indexes for tests\n","train_indexes = random.choices(np.arange(0,len(labels)), k=len(labels)) \n","\n","# Augmentation\n","trans_train = transforms.Compose([transforms.ToPILImage(),\n","                                  transforms.Resize((224, 224)),\n","                                  transforms.RandomHorizontalFlip(), \n","                                  transforms.RandomVerticalFlip(),\n","                                  transforms.RandomRotation(20), \n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n","\n","trans_valid = transforms.Compose([transforms.ToPILImage(),\n","                                  transforms.Resize((224, 224)),\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTNs_vFC96lz"},"source":["## Split"]},{"cell_type":"code","metadata":{"id":"nP6FsbO3s_P2","executionInfo":{"status":"aborted","timestamp":1615242738066,"user_tz":-180,"elapsed":10185,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["\n","batch_size = 20\n","\n","# Разделяем на обучение\\валидацию\\тест\n","train, val = train_test_split(labels.iloc[train_indexes], stratify=[labels.iloc[i][1] for i in train_indexes], test_size=0.2)\n","test,val = train_test_split(val, stratify=val.label, test_size=0.5)\n","\n","\n","\n","\n","train_data = MyDataset(df_data=train, data_dir=train_path, transform=trans_train)\n","valid_data = MyDataset(df_data=val, data_dir=train_path, transform=trans_valid)\n","test_data = MyDataset(df_data=test, data_dir=train_path, transform=trans_valid)\n","\n","\n","#Loaders \n","\n","train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n","valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=False, num_workers=0)\n","test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False, num_workers=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4NdaHt7R-zP"},"source":["## Random Plots"]},{"cell_type":"code","metadata":{"id":"v1olLHjT96l0","executionInfo":{"status":"aborted","timestamp":1615242738066,"user_tz":-180,"elapsed":10176,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# Смотрим на данные\n","random_idx = np.random.randint(1, len(labels.iloc[train_indexes].id), size=9)\n","fig, axes = plt.subplots(3, 3, figsize=(20, 16))\n","for idx, ax in enumerate(axes.ravel()):\n","    \n","    img_path =('train\\\\'+ str(labels.id[idx])+'.tif')\n","    #print(img_path)\n","    img = Image.open(img_path)\n","    ax.set_title(labels.label[idx])\n","    ax.axis('off')\n","    ax.imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EuCqE-m6wDok","executionInfo":{"status":"aborted","timestamp":1615242738067,"user_tz":-180,"elapsed":10168,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["print('Train: ',len(train_data), len(train_loader))\n","print('Test: ',len(test_data), len(test_loader))\n","print('Val: ',len(valid_data), len(valid_loader))\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"swVJTwYZOO0j"},"source":["# Visual Transformer(part 2)\n","\n","## Pretrained VIT\n","\n","### Возьмем стандартную предобученную модель (которой в последующем окажется достаточно)"]},{"cell_type":"code","metadata":{"id":"ot5wNwc7OStY","executionInfo":{"status":"aborted","timestamp":1615242738067,"user_tz":-180,"elapsed":10162,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["!pip install pytorch-pretrained-vit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7Yf3AAiOxEB","executionInfo":{"status":"aborted","timestamp":1615242738068,"user_tz":-180,"elapsed":10156,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["import pytorch_pretrained_vit\n","import torch\n","from vit_pytorch import ViT\n","# model = ViT(\n","#     image_size = 96,\n","#     patch_size = 16,\n","#     num_classes = 2,\n","#     dim = 1024,\n","#     depth = 24,\n","#     heads = 16,\n","#     mlp_dim =4096,\n","#     dropout = 0.2,\n","#     emb_dropout = 0.2\n","# )\n","\n","model_name = 'B_16_imagenet1k'\n","model = pytorch_pretrained_vit.ViT(model_name, image_size=224, pretrained=True)\n","\n","# model = pytorch_pretrained_vit.ViT(model_name,dim = 2048, ff_dim= 1576,num_heads= 1 , patches = 8, num_layers=4,attention_dropout_rate = 0.3, \n","#                                    dropout_rate = 0.5, image_size=90, pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"48D1OgNBJCBJ","executionInfo":{"status":"aborted","timestamp":1615242738068,"user_tz":-180,"elapsed":10149,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["from termcolor import colored\n","\n","colors = ['red', 'green', 'blue', 'yellow']\n","\n","def model_structure(layer, margin=0, item_color=0, deep=0, max_deep=2):\n","    for name, next_layer in layer.named_children():\n","        if deep > max_deep:\n","            return\n","        next = (0 if not list(next_layer.named_children()) else 1)\n","        print(colored(' ' * margin + name, colors[item_color]) + ':' * next)\n","        model_structure(next_layer, margin + len(name) + 2, (item_color + 1) % 4, deep + 1, max_deep=max_deep)\n","\n","model_structure(model, max_deep=2)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4rOeIbvZU_2","executionInfo":{"status":"aborted","timestamp":1615242738069,"user_tz":-180,"elapsed":10143,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["fig, axs = plt.subplots(1, 3, figsize=(14, 6))\n","for i in range(3):\n","    axs[i].pcolormesh(model.patch_embedding.weight.data.cpu().detach().numpy()[0, i])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x46_6zznPsfQ","executionInfo":{"status":"aborted","timestamp":1615242738069,"user_tz":-180,"elapsed":10141,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# change ouput layer\n","# firstly you can train only the last fc layer then finetune all layers\n","\n","#Так же два класса на выходе\n","\n","model.fc = nn.Linear(768, 2)\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyrTlCv6jOWR","executionInfo":{"status":"aborted","timestamp":1615242738070,"user_tz":-180,"elapsed":10134,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["plt.title(\"Learnable embedding for [CLS]\")\n","plt.plot(model.class_token[0, 0].cpu().detach().numpy())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyl65OUujRJ5","executionInfo":{"status":"aborted","timestamp":1615242738070,"user_tz":-180,"elapsed":10127,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["plt.title('Positional embeddings')\n","plt.ylabel('[CLS], Number of the patch')\n","plt.xlabel('Position in vector')\n","plt.pcolormesh(model.positional_embedding.pos_embedding.data[0].cpu().detach().numpy()[:20, :30])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fOY6j0y02zK"},"source":["### Attention до обучения"]},{"cell_type":"code","metadata":{"id":"ykbOEdi2jVr6","executionInfo":{"status":"aborted","timestamp":1615242738071,"user_tz":-180,"elapsed":10121,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["\n","# Подготовка...\n","\n","img_path =('train\\\\'+ str(labels.id[5])+'.tif')# train_data.pathe\n","img = Image.open(img_path)\n","trans1 = transforms.ToTensor()\n","img_transformed = train_data.transform(trans1(img))\n","# our forward\n","x = img_transformed.unsqueeze(0)\n","print(x.shape)\n","model = model.to(device)\n","model.eval()\n","x = model.patch_embedding(x.to(device))\n","print(x.shape)\n","x = x.flatten(2).transpose(1, 2)\n","print(x.shape)\n","x = torch.cat((model.class_token.expand(1, -1, -1), x), dim=1)\n","print(x.shape)\n","x = model.transformer(x)\n","print(x.shape)\n","att_mat = []\n","for block in model.transformer.blocks:\n","    att_mat.append(block.attn.scores.squeeze(0))\n","    \n","# Average the attention weights across all heads.\n","att_mat = torch.mean(torch.stack(att_mat), dim=1)\n","# # To account for residual connections, we add an identity matrix to the\n","# # attention matrix and re-normalize the weights.\n","residual_att = torch.eye(att_mat.size(1)).to(device)\n","\n","aug_att_mat = att_mat + residual_att\n","aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1, keepdim=True)\n","#aug_att_mat.shape\n","# Recursively multiply the weight matrices\n","joint_attentions = torch.zeros_like(aug_att_mat).to(device)\n","# copy first layer\n","joint_attentions[0] = aug_att_mat[0]\n","for n in range(1, aug_att_mat.size(0)):\n","    joint_attentions[n] = aug_att_mat[n] @ joint_attentions[n - 1]\n","    \n","# Attention from the output token to the input space, last layer\n","v = joint_attentions[-1]\n","v.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yDU8_qPoG4g","executionInfo":{"status":"aborted","timestamp":1615242738072,"user_tz":-180,"elapsed":10115,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["#Все еще подготовка...\n","\n","att_mat = []\n","for block in model.transformer.blocks:\n","    att_mat.append(block.attn.scores.squeeze(0))\n","    \n","# Average the attention weights across all heads.\n","att_mat = torch.mean(torch.stack(att_mat), dim=1)\n","# # To account for residual connections, we add an identity matrix to the\n","# # attention matrix and re-normalize the weights.\n","residual_att = torch.eye(att_mat.size(1)).to(device)\n","\n","aug_att_mat = att_mat + residual_att\n","aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1, keepdim=True)\n","#aug_att_mat.shape\n","# Recursively multiply the weight matrices\n","joint_attentions = torch.zeros_like(aug_att_mat).to(device)\n","# copy first layer\n","joint_attentions[0] = aug_att_mat[0]\n","for n in range(1, aug_att_mat.size(0)):\n","    joint_attentions[n] = aug_att_mat[n] @ joint_attentions[n - 1]\n","    \n","# Attention from the output token to the input space, last layer\n","v = joint_attentions[-1]\n","v.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NqtnYjoOKHp","scrolled":false,"executionInfo":{"status":"aborted","timestamp":1615242738073,"user_tz":-180,"elapsed":10107,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# Распределение по головам \n","grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n","mask = v[0, 1:].reshape(grid_size, grid_size).cpu().detach().numpy()\n","mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n","\n","\n","\n","for i, v in enumerate(joint_attentions):\n","    # Attention from the output token to the input space.\n","    mask = v.cpu()[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n","    mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n","    result = (mask * img).astype(\"uint8\")\n","\n","    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n","    ax1.set_title('Original')\n","    ax2.set_title('Attention Map_%d Layer' % (i+1))\n","    _ = ax1.imshow(img)\n","    _ = ax2.imshow(result)\n","\n","\n","# result = (mask * img).astype(\"uint8\")\n","# fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n","\n","# ax1.set_title('Original')\n","# ax2.set_title('Attention Map')\n","# _ = ax1.imshow(img)\n","# _ = ax2.imshow(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4u5YZG1eozIv"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"_hrjlA5aTqm9","executionInfo":{"status":"aborted","timestamp":1615242738073,"user_tz":-180,"elapsed":10105,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# Задаем параметры нейросети\n","params = [param for name, param in model.named_parameters()]\n","# for param in params[:-2]:\n","#      param.requires_grad=True\n","optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n","#optimizer = optim.SGD(params, lr=1, momentum=0.9)\n","scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qx3DLJ3sPu7F","executionInfo":{"status":"aborted","timestamp":1615242738074,"user_tz":-180,"elapsed":10099,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2Ybv6ShNsVp","executionInfo":{"status":"aborted","timestamp":1615242738074,"user_tz":-180,"elapsed":10098,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["def train(model, iterator, optimizer, scheduler, criterion, clip, train_history=None, valid_history=None):\n","    model.train()\n","    \n","    epoch_loss = 0\n","    history = []\n","    i = 0\n","    for data, label in tqdm(iterator):\n","        data = data.to(device)\n","        label = label.to(device)\n","        \n","        output = model(data)\n","        loss = criterion(output, label)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        i += 1\n","        \n","        history.append(loss.cpu().data.numpy())\n","        if (i+1)%20==0:\n","            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n","\n","            clear_output(True)\n","            ax[0].plot(history, label='train loss')\n","            ax[0].set_xlabel('Batch')\n","            ax[0].set_title('Train loss')\n","            if train_history is not None:\n","                ax[1].plot(train_history, label='general train history')\n","                ax[1].set_xlabel('Epoch')\n","            if valid_history is not None:\n","                ax[1].plot(valid_history, label='general valid history')\n","            plt.legend()\n","            \n","            plt.show()\n","\n","    scheduler.step()\n","    \n","    return epoch_loss / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    epoch_loss = 0\n","    history = []\n","    with torch.no_grad():\n","    \n","        for data, label in iterator:\n","            data = data.to(device)\n","            label = label.to(device)\n","\n","            output = model(data)\n","            loss = criterion(output, label)\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUU-m2kLNvmA","executionInfo":{"status":"aborted","timestamp":1615242738075,"user_tz":-180,"elapsed":10097,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["import time\n","import math\n","import matplotlib\n","matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nW35haMp96l5"},"source":["### Графики не сохранились, только веса"]},{"cell_type":"code","metadata":{"id":"FI8WsQkMQEn8","executionInfo":{"status":"aborted","timestamp":1615242738075,"user_tz":-180,"elapsed":10096,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# Обучение\n","\n","train_history = []\n","valid_history = []\n","\n","N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_loader, optimizer, scheduler, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(model, valid_loader, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'best-val-model.pt')\n","    \n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kl7r45-FUmfC","executionInfo":{"status":"aborted","timestamp":1615242738075,"user_tz":-180,"elapsed":10095,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# submit = pd.read_csv('./sample_submission.csv')\n","# submit.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kEKbG71r96l5"},"source":["#### Смотрим качество"]},{"cell_type":"code","metadata":{"id":"20ZJm26lvgn7","executionInfo":{"status":"aborted","timestamp":1615242738076,"user_tz":-180,"elapsed":10088,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["model.load_state_dict(torch.load('ViT_cancer_model'))       #!!!!!\n","best_model = model\n","best_model.eval()\n","\n","\n","pred_labels = []\n","true_labels = []\n","epoch_loss = 0\n","\n","with torch.no_grad():\n","     for data, label in tqdm(valid_loader):\n","        data = data.to(device)\n","        true_labels.append(label.numpy())\n","\n","        output = model(data)\n","        loss = criterion(output, label.to(device))\n","        epoch_loss += loss.item()\n","        pred_labels.append(output.argmax(dim=1).cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuIzReb3GGjk","executionInfo":{"status":"aborted","timestamp":1615242738076,"user_tz":-180,"elapsed":10081,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["from sklearn.metrics import accuracy_score\n","\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_labels = np.concatenate(pred_labels, axis=0)\n","print(f'Accuracy score: {accuracy_score(true_labels, pred_labels)}')\n","print(f\"Loss: {epoch_loss / len(valid_loader)}\")\n","# .9674589828659729"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vz9ZyTNE96l6"},"source":["### Valid accuracy: 0.97"]},{"cell_type":"code","metadata":{"id":"fjqfWzjvw1VU","executionInfo":{"status":"aborted","timestamp":1615242738077,"user_tz":-180,"elapsed":10075,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["\n","best_model = model\n","best_model.eval()\n","\n","\n","prob = []\n","true_labels = []\n","\n","with torch.no_grad():\n","     for data, label in tqdm(test_loader):\n","        data = data.to(device)\n","        true_labels.append(label.numpy())\n","        output = model(data)\n","        prob.append(output.argmax(dim=1).cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vw_kYBy5bLo","executionInfo":{"status":"aborted","timestamp":1615242738077,"user_tz":-180,"elapsed":10067,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["acc = 0\n","for i in range(len(prob)):\n","  acc= acc + accuracy_score(true_labels[i],prob[i])\n","\n","print (acc/len(prob))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNE3Zkke96l6"},"source":["### Test accuracy: 0.96"]},{"cell_type":"code","metadata":{"id":"cNjnatVOrPQu","executionInfo":{"status":"aborted","timestamp":1615242738078,"user_tz":-180,"elapsed":10067,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["#torch.save(model.state_dict(), \"C:\\\\Users\\\\safiu\\\\ViT_cancer_model\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-se0aA03oiU5","executionInfo":{"status":"aborted","timestamp":1615242738078,"user_tz":-180,"elapsed":10059,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["plt.title(\"Learnable embedding for [CLS]\")\n","plt.plot(model.class_token[0, 0].cpu().detach().numpy())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSf62r0GU9zm","executionInfo":{"status":"aborted","timestamp":1615242738079,"user_tz":-180,"elapsed":10058,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["for batch in train_loader:\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fSW39oDU9zo","executionInfo":{"status":"aborted","timestamp":1615242738079,"user_tz":-180,"elapsed":10051,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# our forward\n","print(batch[0].shape)\n","model.eval()\n","x = model.patch_embedding(batch[0].to(device))\n","print(x.shape)\n","x = x.flatten(2).transpose(1, 2)\n","print(x.shape)\n","x = torch.cat((model.class_token.expand(batch_size, -1, -1), x), dim=1)\n","print(x.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H38DdtOJVzZN","executionInfo":{"status":"aborted","timestamp":1615242738080,"user_tz":-180,"elapsed":10045,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["model_old = pytorch_pretrained_vit.ViT(model_name,image_size=224,num_classes = 2, pretrained=True).to(device)\n","model_old.eval()\n","pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mKEWdToU9zq","executionInfo":{"status":"aborted","timestamp":1615242738080,"user_tz":-180,"elapsed":10038,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["plt.title(\"Difference between old and new Learnable embedding for [CLS]\")\n","plt.plot((model.class_token[0, 0] - model_old.class_token[0, 0]).cpu().detach().numpy())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOgivlHDqETi","executionInfo":{"status":"aborted","timestamp":1615242738081,"user_tz":-180,"elapsed":10031,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["model_old.positional_embedding.pos_embedding.data.shape\n","model.positional_embedding.pos_embedding.data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NcEaC8oTU9zr","executionInfo":{"status":"aborted","timestamp":1615242738081,"user_tz":-180,"elapsed":10024,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["plt.title('Difference Positional embeddings')\n","plt.ylabel('[CLS], Number of the patch')\n","plt.xlabel('Position in vector')\n","plt.pcolormesh((model_old.positional_embedding.pos_embedding.data - model.positional_embedding.pos_embedding.data).cpu()[0].detach().numpy()[:20, :30])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QjzPDr2996l8"},"source":["## Смотрим как поменялся attention после обучения"]},{"cell_type":"code","metadata":{"id":"po1i0ei6ZUh_","scrolled":false,"executionInfo":{"status":"aborted","timestamp":1615242738082,"user_tz":-180,"elapsed":10017,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["model_old = pytorch_pretrained_vit.ViT(model_name,image_size=224, pretrained=True, num_classes = 2).to(device)\n","model_old.eval()\n","pass\n","img_path = ('train\\\\'+ str(labels.id[114])+'.tif')\n","img = Image.open(img_path)\n","trans1 = transforms.ToTensor()\n","img_transformed = train_data.transform(trans1(img))\n","# our forward\n","x = img_transformed.unsqueeze(0)\n","print(x.shape)\n","model_old = model_old.to(device)\n","model_old.eval()\n","x = model_old.patch_embedding(x.to(device))\n","print(x.shape)\n","x = x.flatten(2).transpose(1, 2)\n","print(x.shape)\n","x = torch.cat((model_old.class_token.expand(1, -1, -1), x), dim=1)\n","print(x.shape)\n","x = model_old.transformer(x)\n","print(x.shape)\n","att_mat = []\n","for block in model_old.transformer.blocks:\n","    att_mat.append(block.attn.scores.squeeze(0))\n","    \n","# Average the attention weights across all heads.\n","att_mat = torch.mean(torch.stack(att_mat), dim=1)\n","# # To account for residual connections, we add an identity matrix to the\n","# # attention matrix and re-normalize the weights.\n","residual_att = torch.eye(att_mat.size(1)).to(device)\n","\n","aug_att_mat = att_mat + residual_att\n","aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1, keepdim=True)\n","#aug_att_mat.shape\n","# Recursively multiply the weight matrices\n","joint_attentions = torch.zeros_like(aug_att_mat).to(device)\n","# copy first layer\n","joint_attentions[0] = aug_att_mat[0]\n","for n in range(1, aug_att_mat.size(0)):\n","    joint_attentions[n] = aug_att_mat[n] @ joint_attentions[n - 1]\n","    \n","# Attention from the output token to the input space, last layer\n","v = joint_attentions[-1]\n","v.shape\n","grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n","mask = v[0, 1:].reshape(grid_size, grid_size).cpu().detach().numpy()\n","mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n","result = (mask * img).astype(\"uint8\")\n","fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n","\n","ax1.set_title('Original')\n","ax2.set_title('Attention Map_pretrained')\n","_ = ax1.imshow(img)\n","_ = ax2.imshow(result)\n","\n","model = pytorch_pretrained_vit.ViT(model_name,image_size=224, pretrained=True, num_classes =2 ).to(device)\n","\n","model.load_state_dict(torch.load('ViT_cancer_model'))\n","\n","\n","#img_path = ('train\\\\'+ str(labels.id[95])+'.tif')\n","img = Image.open(img_path)\n","trans1 = transforms.ToTensor()\n","img_transformed = train_data.transform(trans1(img))\n","# our forward\n","x = img_transformed.unsqueeze(0)\n","print(x.shape)\n","model = model.to(device)\n","model.eval()\n","x = model.patch_embedding(x.to(device))\n","print(x.shape)\n","x = x.flatten(2).transpose(1, 2)\n","print(x.shape)\n","x = torch.cat((model.class_token.expand(1, -1, -1), x), dim=1)\n","print(x.shape)\n","x = model.transformer(x)\n","print(x.shape)\n","att_mat = []\n","for block in model.transformer.blocks:\n","    att_mat.append(block.attn.scores.squeeze(0))\n","    \n","# Average the attention weights across all heads.\n","att_mat = torch.mean(torch.stack(att_mat), dim=1)\n","# # To account for residual connections, we add an identity matrix to the\n","# # attention matrix and re-normalize the weights.\n","residual_att = torch.eye(att_mat.size(1)).to(device)\n","\n","aug_att_mat = att_mat + residual_att\n","aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1, keepdim=True)\n","#aug_att_mat.shape\n","# Recursively multiply the weight matrices\n","joint_attentions = torch.zeros_like(aug_att_mat).to(device)\n","# copy first layer\n","joint_attentions[0] = aug_att_mat[0]\n","for n in range(1, aug_att_mat.size(0)):\n","    joint_attentions[n] = aug_att_mat[n] @ joint_attentions[n - 1]\n","    \n","# Attention from the output token to the input space, last layer\n","v = joint_attentions[-1]\n","v.shape\n","grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n","mask = v[0, 1:].reshape(grid_size, grid_size).cpu().detach().numpy()\n","mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n","result1 = (mask * img).astype(\"uint8\")\n","fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n","\n","ax1.set_title('Original')\n","ax2.set_title('Attention Map_transferL')\n","_ = ax1.imshow(img)\n","_ = ax2.imshow(result1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjdbVJ-w96l9"},"source":["### Я конечно не биолог, но кажется, что после обучения attention стал обращать внимание на более важные области патологии"]},{"cell_type":"markdown","metadata":{"id":"OqO5wJpGPSBm"},"source":["# Visual Transformer(part 3)\n","\n","## CNN test"]},{"cell_type":"code","metadata":{"id":"A7Zzd2ciHNxK","executionInfo":{"status":"aborted","timestamp":1615242738082,"user_tz":-180,"elapsed":10015,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["train_dir = 'train'\n","test_dir = 'test'\n","\n","train_indexes = random.choices(np.arange(0,len(labels)), k=len(labels))\n","trainD, val = train_test_split(labels.iloc[train_indexes], stratify=[labels.iloc[i][1] for i in train_indexes], test_size=0.2)\n","test,val = train_test_split(val, stratify=val.label, test_size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPNw4X__rmqY","executionInfo":{"status":"aborted","timestamp":1615242738083,"user_tz":-180,"elapsed":10015,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUR_rOA0zzAZ","executionInfo":{"status":"aborted","timestamp":1615242738083,"user_tz":-180,"elapsed":10013,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# Параметры для CNN\n","\n","num_classes = 2\n","batch_size = 110\n","learning_rate = 0.002\n","\n","# Device configuration\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9QQh8F4z1jl","executionInfo":{"status":"aborted","timestamp":1615242738083,"user_tz":-180,"elapsed":10012,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["#Повторяем подготовительный процесс\n","\n","train_data = MyDataset(df_data=trainD, data_dir=train_path, transform=trans_train)\n","test_data = MyDataset(df_data=test, data_dir=train_path, transform=trans_valid)\n","valid_data = MyDataset(df_data=val, data_dir=train_path, transform=trans_valid)\n","\n","\n","\n","\n","train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True, num_workers=0)\n","valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","\n","\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=2)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=2)\n","        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=2)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.bn5 = nn.BatchNorm2d(512)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.avg = nn.AvgPool2d(8)\n","        self.fc = nn.Linear(512 * 1 * 1, 2) # !!!\n","    def forward(self, x):\n","        #print (x.shape)\n","        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x)))) \n","        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n","        x = self.pool(F.leaky_relu(self.bn4(self.conv4(x))))\n","        x = self.pool(F.leaky_relu(self.bn5(self.conv5(x))))\n","        x = self.avg(x)\n","        x = x.view(-1, 512 * 1 * 1) # !!!\n","        x = self.fc(x)\n","        return x\n","\n","model = SimpleCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYD_-3jhz8X_","scrolled":false,"executionInfo":{"status":"aborted","timestamp":1615242738084,"user_tz":-180,"elapsed":10005,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["train_history = []\n","valid_history = []\n","import time\n","import math\n","import matplotlib\n","matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import clear_output\n","N_EPOCHS = 10\n","CLIP = 1\n","scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n","import cv2\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_loader, optimizer, scheduler, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(model, valid_loader, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'best-val-model_CNN.pt')\n","    \n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiCwBEnr96mA","executionInfo":{"status":"aborted","timestamp":1615242738084,"user_tz":-180,"elapsed":9996,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["# Картинка с обучения\n","from IPython.display import Image\n","from IPython.core.display import HTML \n","Image(url= \"https://sun9-64.userapi.com/impg/MlzY0Rjlv63SmwOH0qegh0t2iiC-HrrhTthHrA/KBIsALrXMP4.jpg?size=719x496&quality=96&proxy=1&sign=8bbeb100f5eb36802c9f22afe9e103c3&type=album\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1V_CKRlm0CQ1","executionInfo":{"status":"aborted","timestamp":1615242738085,"user_tz":-180,"elapsed":9996,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRKnqLUTz83H","executionInfo":{"status":"aborted","timestamp":1615242738087,"user_tz":-180,"elapsed":9990,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["best_model = model\n","best_model.load_state_dict(torch.load('CNN_cancer_model11'))           #!!!!\n","\n","best_model.eval()\n","\n","\n","pred_labels = []\n","true_labels = []\n","epoch_loss = 0\n","\n","with torch.no_grad():\n","     for data, label in tqdm(valid_loader):\n","        data = data.to(device)\n","        true_labels.append(label.numpy())\n","\n","        output = model(data)\n","        loss = criterion(output, label.to(device))\n","        epoch_loss += loss.item()\n","        pred_labels.append(output.argmax(dim=1).cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqxocd0yz83J","executionInfo":{"status":"aborted","timestamp":1615242738087,"user_tz":-180,"elapsed":9982,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["from sklearn.metrics import accuracy_score\n","\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_labels = np.concatenate(pred_labels, axis=0)\n","print(f'Accuracy score: {accuracy_score(true_labels, pred_labels)}')\n","print(f\"Loss: {epoch_loss / len(valid_loader)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eoZljqh_96mB"},"source":["### Valid accuracy: 0.95"]},{"cell_type":"code","metadata":{"id":"_XJSMKFGz9Uw","executionInfo":{"status":"aborted","timestamp":1615242738088,"user_tz":-180,"elapsed":9975,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["\n","best_model = model\n","best_model.eval()\n","\n","\n","prob = []\n","true_labels = []\n","\n","with torch.no_grad():\n","     for data, label in tqdm(test_loader):\n","        data = data.to(device)\n","        true_labels.append(label.numpy())\n","        output = model(data)\n","        prob.append(output.argmax(dim=1).cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGuj7Orq96mB","executionInfo":{"status":"aborted","timestamp":1615242738088,"user_tz":-180,"elapsed":9968,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["acc = 0\n","for i in range(len(prob)):\n","    acc= acc + accuracy_score(true_labels[i],prob[i])\n","\n","print (acc/len(prob))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLBaO_3g96mC"},"source":["### Test accuracy: 0.94"]},{"cell_type":"code","metadata":{"id":"of3sNZk-96mC","executionInfo":{"status":"aborted","timestamp":1615242738089,"user_tz":-180,"elapsed":9967,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":["#torch.save(model.state_dict(), \"C:\\\\Users\\\\safiu\\\\CNN_cancer_model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwH5G1xa96mC"},"source":["### Таким образом получаем, что наш не сильно большой и отлаженный ViT получился лучше неплохой CNN"]},{"cell_type":"code","metadata":{"id":"fqxQMJ9f96mC","executionInfo":{"status":"aborted","timestamp":1615242738089,"user_tz":-180,"elapsed":9966,"user":{"displayName":"Роберт Русланович Сафиуллин","photoUrl":"","userId":"07536939136464736704"}}},"source":[""],"execution_count":null,"outputs":[]}]}